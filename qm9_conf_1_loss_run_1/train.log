Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  model_dim: 25
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 1
  n_model_confs: 1
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
num_node_features: 44
num_edge_features: 4


Starting training...
Epoch 1: Training Loss -0.6161666269183159
Epoch 1: Validation Loss -0.27449024309005055
Epoch 2: Training Loss -0.975455684775114
Epoch 2: Validation Loss -1.128886013750046
Epoch 3: Training Loss -1.168724786567688
Epoch 3: Validation Loss -1.1934068543570382
Epoch 4: Training Loss -1.200354930114746
Epoch 4: Validation Loss -1.2735090085438319
Epoch 5: Training Loss -1.2448836055755614
Epoch 5: Validation Loss -1.258963458121769
Epoch 6: Training Loss -1.2622051374435426
Epoch 6: Validation Loss -1.2823410109868125
Epoch 7: Training Loss -1.2882102674484253
Epoch 7: Validation Loss -1.3250223511741275
Epoch 8: Training Loss -1.3088433626174927
Epoch 8: Validation Loss -1.321593821994842
Epoch 9: Training Loss -1.3208107349395752
Epoch 9: Validation Loss -1.3610954909097581
Epoch 10: Training Loss -1.3600560733795166
Epoch 10: Validation Loss -1.3663048138694158
Epoch 11: Training Loss -1.383931886100769
Epoch 11: Validation Loss -1.3627598758727786
Epoch 12: Training Loss -1.3957147481918335
Epoch 12: Validation Loss -1.4126901475210039
Epoch 13: Training Loss -1.4118416931152344
Epoch 13: Validation Loss -1.4143228360584803
Epoch 14: Training Loss -1.4192391300201417
Epoch 14: Validation Loss -1.4175123506122165
Epoch 15: Training Loss -1.4297790479660035
Epoch 15: Validation Loss -1.3864403508958363
Epoch 16: Training Loss -1.4272381805419923
Epoch 16: Validation Loss -1.4312659956160045
Epoch 17: Training Loss -1.4386802642822265
Epoch 17: Validation Loss -1.466366103717259
Epoch 18: Training Loss -1.4510853584289551
Epoch 18: Validation Loss -1.3865129190777976
Epoch 19: Training Loss -1.4645839654922486
Epoch 19: Validation Loss -1.4591055048836603
Epoch 20: Training Loss -1.4749372791290283
Epoch 20: Validation Loss -1.4696315375585405
Epoch 21: Training Loss -1.482539584350586
Epoch 21: Validation Loss -1.5005973407200404
Epoch 22: Training Loss -1.4844214347839355
Epoch 22: Validation Loss -1.5162329579156542
Epoch 23: Training Loss -1.472009496307373
Epoch 23: Validation Loss -1.4220856825510662
Epoch 24: Training Loss -1.4930650421142577
Epoch 24: Validation Loss -1.5030704755631705
Epoch 25: Training Loss -1.4933812732696534
Epoch 25: Validation Loss -1.5261155234442816
Epoch 26: Training Loss -1.5111048250198365
Epoch 26: Validation Loss -1.5277964860673934
Epoch 27: Training Loss -1.5027074829101563
Epoch 27: Validation Loss -1.5412567702550737
Epoch 28: Training Loss -1.5089775728225707
Epoch 28: Validation Loss -1.4770929321410164
Epoch 29: Training Loss -1.5113447536468505
Epoch 29: Validation Loss -1.5065088291016837
Epoch 30: Training Loss -1.5242838390350342
Epoch 30: Validation Loss -1.5203512036611164
Epoch 31: Training Loss -1.516748069190979
Epoch 31: Validation Loss -1.5031777771692427
Epoch 32: Training Loss -1.4929474061965942
Epoch 32: Validation Loss -1.4747878596896218
Epoch 33: Training Loss -1.4984380250930787
Epoch 33: Validation Loss -1.5020625288524325
Epoch 34: Training Loss -1.5420909763336181
Epoch 34: Validation Loss -1.5736237310227894
Epoch 35: Training Loss -1.5452504627227783
Epoch 35: Validation Loss -1.5481666496821813
Epoch 36: Training Loss -1.5415584257125854
Epoch 36: Validation Loss -1.5266200247265043
Epoch 37: Training Loss -1.547949369621277
Epoch 37: Validation Loss -1.530268945391216
Epoch 38: Training Loss -1.5450225341796875
Epoch 38: Validation Loss -1.5640094431619795
Epoch 39: Training Loss -1.5620088582992553
Epoch 39: Validation Loss -1.5499370457634094
Epoch 40: Training Loss -1.545912883758545
Epoch 40: Validation Loss -1.55315543545617
Epoch 41: Training Loss -1.5821447912216187
Epoch 41: Validation Loss -1.5745577850039043
Epoch 42: Training Loss -1.5805548797607423
Epoch 42: Validation Loss -1.5616271117376903
Epoch 43: Training Loss -1.583309276008606
Epoch 43: Validation Loss -1.5842439201143053
Epoch 44: Training Loss -1.5813616483688355
Epoch 44: Validation Loss -1.5625160894696675
Epoch 45: Training Loss -1.5798433729171752
Epoch 45: Validation Loss -1.5757237388974143
Epoch 46: Training Loss -1.582396729660034
Epoch 46: Validation Loss -1.5640987744407049
Epoch 47: Training Loss -1.5786787782669067
Epoch 47: Validation Loss -1.5841167673232064
Epoch 48: Training Loss -1.5874222341537476
Epoch 48: Validation Loss -1.5966061202306596
Epoch 49: Training Loss -1.5900540910720826
Epoch 49: Validation Loss -1.5933535761303372
Epoch 50: Training Loss -1.5889411531448365
Epoch 50: Validation Loss -1.5949323253026084
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  model_dim: 25
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 1
  n_model_confs: 1
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
num_node_features: 44
num_edge_features: 4


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  model_dim: 25
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 1
  n_model_confs: 1
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
num_node_features: 44
num_edge_features: 4


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  model_dim: 25
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 1
  n_model_confs: 1
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
num_node_features: 44
num_edge_features: 4


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  model_dim: 25
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 1
  n_model_confs: 1
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
num_node_features: 44
num_edge_features: 4


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  model_dim: 25
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 1
  n_model_confs: 1
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
num_node_features: 44
num_edge_features: 4


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  model_dim: 25
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 1
  n_model_confs: 1
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
num_node_features: 44
num_edge_features: 4


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 1
warmup_epochs: 2
batch_size: 1
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
Best Validation Loss inf on Epoch 0
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 10
warmup_epochs: 2
batch_size: 1
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
Arguments are...
log_dir: ./qm9_conf_1_loss_run_1
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: ./qm9_conf_1_run_1
dataset: qm9
seed: 0
n_epochs: 100
warmup_epochs: 2
batch_size: 1
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  encoder:
    n_head: 2
  global_transformer: False
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  h_mol_mlp:
    n_layers: 1
  loss_type: ot_emd
  model_dim: 25
  n_model_confs: 1
  n_true_confs: 1
  random_alpha: False
  random_vec_dim: 10
  random_vec_std: 1
  teacher_force: False
num_edge_features: 4
num_node_features: 44


Starting training...
