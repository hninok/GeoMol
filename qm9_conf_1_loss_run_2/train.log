Arguments are...
log_dir: ./qm9_conf_1_loss_run_2
data_dir: data/QM9/qm9/
split_path: data/QM9/splits/split0.npy
trained_local_model: None
restart_dir: None
dataset: qm9
seed: 0
n_epochs: 250
warmup_epochs: 2
batch_size: 16
lr: 0.001
num_workers: 0
optimizer: adam
scheduler: plateau
verbose: False
model_dim: 25
random_vec_dim: 10
random_vec_std: 1
random_alpha: False
n_true_confs: 1
n_model_confs: 1
gnn1_depth: 3
gnn1_n_layers: 2
gnn2_depth: 3
gnn2_n_layers: 2
encoder_n_head: 2
coord_pred_n_layers: 2
d_mlp_n_layers: 1
h_mol_mlp_n_layers: 1
alpha_mlp_n_layers: 2
c_mlp_n_layers: 1
global_transformer: False
loss_type: ot_emd
teacher_force: False
separate_opts: False

Model parameters are:
hyperparams:
  model_dim: 25
  random_vec_dim: 10
  random_vec_std: 1
  global_transformer: False
  n_true_confs: 1
  n_model_confs: 1
  gnn1:
    depth: 3
    n_layers: 2
  gnn2:
    depth: 3
    n_layers: 2
  encoder:
    n_head: 2
  coord_pred:
    n_layers: 2
  d_mlp:
    n_layers: 1
  h_mol_mlp:
    n_layers: 1
  alpha_mlp:
    n_layers: 2
  c_mlp:
    n_layers: 1
  loss_type: ot_emd
  teacher_force: False
  random_alpha: False
num_node_features: 44
num_edge_features: 4


Starting training...
Epoch 1: Training Loss -0.6140134975522756
Epoch 1: Validation Loss -0.3241542826687533
Epoch 2: Training Loss -1.0108781347870828
Epoch 2: Validation Loss -1.1389712246637496
Epoch 3: Training Loss -1.1958442109107972
Epoch 3: Validation Loss -1.2016479060763405
Epoch 4: Training Loss -1.2299521866798402
Epoch 4: Validation Loss -1.2616633801233201
Epoch 5: Training Loss -1.2560435395240783
Epoch 5: Validation Loss -1.1946397698114788
Epoch 6: Training Loss -1.2826472965240479
Epoch 6: Validation Loss -1.3262849440650335
Epoch 7: Training Loss -1.308789906311035
Epoch 7: Validation Loss -1.338893004826137
Epoch 8: Training Loss -1.3549706380844115
Epoch 8: Validation Loss -1.3578871053362649
Epoch 9: Training Loss -1.380356714630127
Epoch 9: Validation Loss -1.3677479217922877
Epoch 10: Training Loss -1.392375625228882
Epoch 10: Validation Loss -1.3902236393519811
Epoch 11: Training Loss -1.4001988302230834
Epoch 11: Validation Loss -1.4281831241789318
Epoch 12: Training Loss -1.3940315488815307
Epoch 12: Validation Loss -1.432712405446976
Epoch 13: Training Loss -1.409916029548645
Epoch 13: Validation Loss -1.41060837866768
Epoch 14: Training Loss -1.4160663204193116
Epoch 14: Validation Loss -1.3944936990737915
Epoch 15: Training Loss -1.4245743619918823
Epoch 15: Validation Loss -1.4508611455796256
Epoch 16: Training Loss -1.429826526260376
Epoch 16: Validation Loss -1.3977606428994074
Epoch 17: Training Loss -1.445112343597412
Epoch 17: Validation Loss -1.4336787273013403
Epoch 18: Training Loss -1.457863235092163
Epoch 18: Validation Loss -1.47554944431971
Epoch 19: Training Loss -1.463723540687561
Epoch 19: Validation Loss -1.444936303865342
Epoch 20: Training Loss -1.464096975517273
Epoch 20: Validation Loss -1.4344900789714994
Epoch 21: Training Loss -1.4600791090011596
Epoch 21: Validation Loss -1.4493417228971208
Epoch 22: Training Loss -1.4573869583129884
Epoch 22: Validation Loss -1.4509867600032262
Epoch 23: Training Loss -1.4681963176727295
Epoch 23: Validation Loss -1.4856303513996185
Epoch 24: Training Loss -1.4744929851531983
Epoch 24: Validation Loss -1.5108645624584622
Epoch 25: Training Loss -1.4573849439620972
Epoch 25: Validation Loss -1.4283857345581055
Epoch 26: Training Loss -1.474454688835144
Epoch 26: Validation Loss -1.4999570997934493
Epoch 27: Training Loss -1.4969851480484009
Epoch 27: Validation Loss -1.524709421490866
Epoch 28: Training Loss -1.4937180238723755
Epoch 28: Validation Loss -1.491692991483779
Epoch 29: Training Loss -1.4919338438034058
Epoch 29: Validation Loss -1.4794451414592682
Epoch 30: Training Loss -1.4817203235626222
Epoch 30: Validation Loss -1.4797246134470379
Epoch 31: Training Loss -1.4968379791259765
Epoch 31: Validation Loss -1.5258290748747567
Epoch 32: Training Loss -1.5066355075836182
Epoch 32: Validation Loss -1.5168736189130754
Epoch 33: Training Loss -1.4833618879318238
Epoch 33: Validation Loss -1.5126828522909255
Epoch 34: Training Loss -1.5085098718643188
Epoch 34: Validation Loss -1.5284405738588362
Epoch 35: Training Loss -1.5120535900115968
Epoch 35: Validation Loss -1.4940042949858165
Epoch 36: Training Loss -1.5106879486083984
Epoch 36: Validation Loss -1.5227401710691906
Epoch 37: Training Loss -1.507049462890625
Epoch 37: Validation Loss -1.5277974984002491
Epoch 38: Training Loss -1.5134599668502808
Epoch 38: Validation Loss -1.5057156161656455
Epoch 39: Training Loss -1.5029533563613893
Epoch 39: Validation Loss -1.499296716281346
Epoch 40: Training Loss -1.504359296798706
Epoch 40: Validation Loss -1.5135010102438549
Epoch 41: Training Loss -1.538501544380188
Epoch 41: Validation Loss -1.5522256143509396
Epoch 42: Training Loss -1.5367470716476441
Epoch 42: Validation Loss -1.545382121252635
Epoch 43: Training Loss -1.542550478363037
Epoch 43: Validation Loss -1.5409470096467033
Epoch 44: Training Loss -1.5499184211730956
Epoch 44: Validation Loss -1.5587331491803367
Epoch 45: Training Loss -1.553914792060852
Epoch 45: Validation Loss -1.5441241813084436
Epoch 46: Training Loss -1.555574667930603
Epoch 46: Validation Loss -1.5535251677982391
Epoch 47: Training Loss -1.5524615047454835
Epoch 47: Validation Loss -1.5387587452691698
Epoch 48: Training Loss -1.5571580909729004
Epoch 48: Validation Loss -1.5828059135921417
Epoch 49: Training Loss -1.559643437385559
Epoch 49: Validation Loss -1.5665673868996757
Epoch 50: Training Loss -1.5614146505355835
Epoch 50: Validation Loss -1.5634207782291232
